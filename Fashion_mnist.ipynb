{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c83accb",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accepted-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-cassette",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "federal-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to extract data from downloaded files\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "## use your file location\n",
    "X_train, y_train = load_mnist('/Users/princychahal/Documents/MNIST Fashion2', kind='train')\n",
    "X_test, y_test = load_mnist('/Users/princychahal/Documents/MNIST Fashion2', kind='t10k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-insurance",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "collected-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing with scaling\n",
    "\n",
    "def data_scale(X):\n",
    "    X = X/255.0\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opening-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing with normalization\n",
    "\n",
    "def data_normalize(X):\n",
    "    mean = np.mean(X,axis = 0)\n",
    "    std_deviation = np.std(X,axis = 0)\n",
    "    X = (X-mean)/(std_deviation)\n",
    "    return (X,mean,std_deviation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-support",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interracial-little",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000,)\n",
      "(180000, 784)\n"
     ]
    }
   ],
   "source": [
    "# # Data augmentation on training data\n",
    "\n",
    "image = X_train.reshape(60000,28,28)\n",
    "y = y_train[:,np.newaxis]\n",
    "final_x = np.hstack((X_train,y))\n",
    "\n",
    "## horizontal flip on training data\n",
    "hor_image = copy.deepcopy(image)\n",
    "for i in range(len(image)):\n",
    "    a = hor_image[i]\n",
    "    hor_image[i] = np.flip(a,axis=1)\n",
    "hor_image = hor_image.reshape(60000,784)\n",
    "final_hor = np.hstack((hor_image,y))\n",
    "\n",
    "\n",
    "## vertical flip on training data\n",
    "ver_image = copy.deepcopy(image)\n",
    "for i in range(len(image)):\n",
    "    a = ver_image[i]\n",
    "    ver_image[i] = np.flip(a,axis=0)\n",
    "ver_image = ver_image.reshape(60000,784)\n",
    "final_ver = np.hstack((ver_image,y))\n",
    "\n",
    "## copy-paste augmentation on training data\n",
    "cutpaste_image = copy.deepcopy(image)\n",
    "for i in range(len(image)):\n",
    "    j = random.randint(0,60000)\n",
    "    cutpaste_image2 = cutpaste_image[j]\n",
    "    cutpaste_image[i][0:15,14:] = cutpaste_image2[0:15,14:]\n",
    "cutpaste_image = cutpaste_image.reshape(60000,784)\n",
    "final_cutpaste = np.hstack((cutpaste_image,y))\n",
    "\n",
    "\n",
    "\n",
    "## combining all X_train, horizontal flip data and vertical flip data\n",
    "final = np.vstack((final_x,final_hor))\n",
    "final = np.vstack((final,final_ver))\n",
    "np.random.shuffle(final)\n",
    "final_y = final[:,-1]\n",
    "final_X = final[:,:-1]\n",
    "print(final_y.shape)\n",
    "print(final_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lesbian-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "concrete-neighbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000,)\n",
      "(30000, 784)\n"
     ]
    }
   ],
   "source": [
    "## Data augmentation on testing data\n",
    "\n",
    "image_test = X_test.reshape(10000,28,28)\n",
    "y_new = y_test[:,np.newaxis]\n",
    "final_x_test = np.hstack((X_test,y_new))\n",
    "\n",
    "## horizontal flip on testing data\n",
    "hor_test_image = copy.deepcopy(image_test)\n",
    "for i in range(len(image_test)):\n",
    "    a = hor_test_image[i]\n",
    "    hor_test_image[i] = np.flip(a,axis=1)\n",
    "hor_test_image = hor_test_image.reshape(10000,784)\n",
    "final_test_hor = np.hstack((hor_test_image,y_new))\n",
    "\n",
    "\n",
    "## vertical flip on testing data\n",
    "ver_test_image = copy.deepcopy(image_test)\n",
    "for i in range(len(image_test)):\n",
    "    a = ver_test_image[i]\n",
    "    ver_test_image[i] = np.flip(a,axis=0)\n",
    "ver_test_image = ver_test_image.reshape(10000,784)\n",
    "final_test_ver = np.hstack((ver_test_image,y_new))\n",
    "\n",
    "## copy-paste augmentation on testing data\n",
    "cutpaste_test_image = copy.deepcopy(image_test)\n",
    "for i in range(len(image_test)):\n",
    "    j = random.randint(0,10000)\n",
    "    cutpaste_test_image2 = cutpaste_test_image[j]\n",
    "    cutpaste_test_image[i][0:15,14:] = cutpaste_test_image2[0:15,14:]\n",
    "cutpaste_test_image = cutpaste_test_image.reshape(10000,784)\n",
    "final_test_cutpaste = np.hstack((cutpaste_test_image,y))\n",
    "\n",
    "## combining all X_test, horizontal flip and vertical flip data\n",
    "test_data = np.vstack((final_x_test,final_test_hor))\n",
    "test_data = np.vstack((test_data,final_test_ver))\n",
    "np.random.shuffle(test_data)\n",
    "y_data_test = test_data[:,-1]\n",
    "X_data_test = test_data[:,:-1]\n",
    "print(y_data_test.shape)\n",
    "print(X_data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-metabolism",
   "metadata": {},
   "source": [
    "## Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pointed-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "arranged-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making object of classifier\n",
    "model1 = RandomForestClassifier(n_estimators = 100)\n",
    "model2 = RandomForestClassifier(n_estimators = 100)\n",
    "model3 = RandomForestClassifier(n_estimators = 100)\n",
    "model4 = RandomForestClassifier(n_estimators = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afdb40b",
   "metadata": {},
   "source": [
    "Data augmentation on traing and testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "greek-sympathy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8718\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model1.fit(final_X,final_y)\n",
    "y_preds = model1.predict(X_data_test)\n",
    "y_comp = y_data_test-y_preds\n",
    "acc = len(y_comp)-np.count_nonzero(y_comp)\n",
    "acc_percent = acc/(len(y_comp))\n",
    "print(acc_percent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec92038",
   "metadata": {},
   "source": [
    "Scaling+data augmentation on traing and testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "amended-mercy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8709333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaled_data = data_scale(final_X)\n",
    "model2.fit(scaled_data,final_y)\n",
    "scaled_test_data = data_scale(X_data_test)\n",
    "y_preds = model2.predict(scaled_test_data)\n",
    "y_comp = y_data_test-y_preds\n",
    "acc = len(y_comp)-np.count_nonzero(y_comp)\n",
    "acc_percent = acc/(len(y_comp))\n",
    "print(acc_percent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41795f",
   "metadata": {},
   "source": [
    "Normalization+data augmentation on traing and testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "greenhouse-hurricane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8712333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "norm_data,mean,std_deviation = data_normalize(final_X)\n",
    "model3.fit(norm_data,final_y)\n",
    "norm_test_data = (X_data_test-mean)/(std_deviation)\n",
    "y_preds = model3.predict(norm_test_data)\n",
    "y_comp = y_data_test-y_preds\n",
    "acc = len(y_comp)-np.count_nonzero(y_comp)\n",
    "acc_percent = acc/(len(y_comp))\n",
    "print(acc_percent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258ca5c",
   "metadata": {},
   "source": [
    "Scaling+normalization+data augmentation on traing and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "moved-tuesday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8742666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "scale_norm_data,norm_mean,norm_deviation = data_normalize(scaled_data)\n",
    "model4.fit(scale_norm_data,final_y)\n",
    "scaled_test_data = data_scale(X_data_test)\n",
    "scale_norm_test_data = (scaled_test_data-norm_mean)/(norm_deviation)\n",
    "y_preds = model4.predict(scale_norm_test_data)\n",
    "y_comp = y_data_test-y_preds\n",
    "acc = len(y_comp)-np.count_nonzero(y_comp)\n",
    "acc_percent = acc/(len(y_comp))\n",
    "print(acc_percent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da8b0e",
   "metadata": {},
   "source": [
    "Data augmentation on testing data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "durable-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6110333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model5 = RandomForestClassifier(n_estimators = 100)\n",
    "model5.fit(X_train,y_train)\n",
    "y_preds = model5.predict(X_data_test)\n",
    "y_comp = y_data_test-y_preds\n",
    "acc = len(y_comp)-np.count_nonzero(y_comp)\n",
    "acc_percent = acc/(len(y_comp))\n",
    "print(acc_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f4b670",
   "metadata": {},
   "source": [
    "Normalization + Scaling + data augmentation on testing data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "increased-holly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6091666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model6 = RandomForestClassifier(n_estimators = 100)\n",
    "scaled_org_data = data_scale(X_train)\n",
    "scale_norm_org_data,norm_org_mean,norm_org_deviation = data_normalize(scaled_org_data)\n",
    "model6.fit(scale_norm_org_data,y_train)\n",
    "scaled_test_data = data_scale(X_data_test)\n",
    "\n",
    "scale_norm_org_test_data = (scaled_test_data-norm_org_mean)/(norm_org_deviation)\n",
    "y_preds = model6.predict(scale_norm_org_test_data)\n",
    "y_comp = y_data_test-y_preds\n",
    "acc = len(y_comp)-np.count_nonzero(y_comp)\n",
    "acc_percent = acc/(len(y_comp))\n",
    "print(acc_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-province",
   "metadata": {},
   "source": [
    "## Random Forest Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from dtreeviz.trees import *\n",
    "\n",
    "other_image = X_train.reshape(60000, 28, 28)\n",
    "\n",
    "model1 = RandomForestClassifier(n_estimators=1, max_depth=3)\n",
    "model1.fit(final_X,final_y)\n",
    "y_preds = model1.predict(X_data_test)\n",
    "y_comp = y_data_test-y_preds\n",
    "acc = len(y_comp)-np.count_nonzero(y_comp)\n",
    "acc_percent = acc/(len(y_comp))\n",
    "print(acc_percent)\n",
    "\n",
    "viz = dtreeviz(\n",
    "    model1.estimators_[0],\n",
    "    final_X[:1000],\n",
    "    final_y[:1000],\n",
    "    class_names=[\n",
    "        \"T-shirt/top\",\n",
    "        \"Trouser\",\n",
    "        \"Pullover\",\n",
    "        \"Dress\",\n",
    "        \"Coat\",\n",
    "        \"Sandal\",\n",
    "        \"Shirt\",\n",
    "        \"Sneaker\",\n",
    "        \"Bag\",\n",
    "        \"Ankle boot\"\n",
    "    ],\n",
    "    feature_names=[f\"px{i}\" for i in range(28 * 28)]\n",
    ")\n",
    "\n",
    "## edit the save path\n",
    "viz.save(\"/Users/princychahal/Documents/github/test.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
